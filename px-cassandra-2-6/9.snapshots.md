In this step, we will take a snapshot of all volumes for our Cassandra cluster, then drop our database table.

### Step: Take snapshot using kubectl

First let's insert a new record in our features table so we can show that the snapshot will take the latest available data:
```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
INSERT INTO portworx.features (id, name, value) VALUES ('px-6', '3DSnaps', 'Application/Cluster aware snapshots!');
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

We're going to use STORK to take a 3DSnapshot of our Cassandra cluster. Take a look at the px-snap.yaml file ```cat px-snap.yaml ```{{execute T1}} and notice that we are going to force a ```nodetool flush``` command on each cluster member before we take the snapshot. As explained before, that will force all data to be written to disk in order to ensure consistency of the snapshot. We also defined the volume group name (cassandra_vg) so Portworx will synchronously quiesce I/O on all volumes before triggering their snapshots.

Now let's take a snapshot.
```
kubectl create -f px-snap.yaml
```{{execute T1}}

You can see the snapshots using the following command:
```
kubectl get volumesnapshots
```{{execute T1}}

### Step: Drop features table

Now we're going to go ahead and do something stupid because it's Katacoda and we're here to learn.

```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
DROP TABLE IF EXISTS portworx.features;
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

Ok, so we deleted our database, what now? 

Create clones from your snapshots and restore from those snapshots.

First edit vols-from-snaps and inset the volumesnapshots from the above "kubectl get volumesnapshots" output.

 `vi vols-from-snaps.yaml`{{execute T1}}

 Then create the clones.
```
kubectl create -f vols-from-snaps.yaml
```{{execute T1}}


Restore cassandra. We delete the original Cassandra deployment only because we dont have enough nodes in this lab to host two. Then we create the new cassandra statefulset based on our cloned snapshots.
```
kubectl delete -f cassandra.yaml
kubectl create -f cassandra-app-restore.yaml
```{{execute T1}}

Verify data

Start a CQL Shell session:
```
kubectl exec -it cqlsh-restored -- cqlsh cassandra-restored-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
```{{execute T1}}

Select rows from the keyspace we previously created:
```
SELECT id, name, value FROM portworx.features;
```{{execute T1}}

You have now restored from a snapshot! Go ahead and ```quit```{{execute T1}} the cqlsh session before finishing.




