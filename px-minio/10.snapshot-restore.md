In this step, we will restore the cloud snapshot of our Minio volume and launch a second Minio server with the recovered data.

### Step: Restore the cloud snapshot to a new volume

Create a PVC using the STORK storage class and the name of the cloud snapshot we just used. Take a look at the yaml: ```clear && cat px-snap-pvc.yaml```{{execute T1}} and and then apply it with kubectl:
```
kubectl create -f px-snap-pvc.yaml
```{{execute T1}}

This will take a few seconds, you can get the PVC and wait till it's BOUND:
```
watch kubectl describe pvc minio-snap-clone
```{{execute T1}}

Once the PVC is BOUND you can hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

### Step: Create a new PVC from the Cloud Snapshot and use it to start a new Minio server
Deploy Minio with it by setting persistence.existingClaim on the helm chart.
```
helm install px-minio \
    --set accessKey=myaccesskey \
    --set secretKey=mysecretkey \
    --set persistence.existingClaim=minio-snap-clone \
    minio/minio
```{{execute T1}}

Run the below command and wait for the Minio pod to be in ready state.
```
watch kubectl get pods -o wide
```{{execute T1}}

When the Minio pod is in Running and Ready 1/1 state then hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

### Step: Verify that Minio has our data
Set the endpoint to the px-snap minio svc
```
MINIO_ENDPOINT=$(kubectl describe svc px-snap-minio | grep Endpoints | awk '{print $2}')
mc alias set px http://$MINIO_ENDPOINT myaccesskey mysecretkey
```{{execute T1}}
List the contents of our bucket and notice it is in the state it was at the time of the backup.
```
mc ls --recursive px
```{{execute T1}}
